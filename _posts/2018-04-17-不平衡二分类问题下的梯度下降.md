---
title: 不平衡二分类问题下的梯度下降(1991)
categories: 
- 论文笔记之不平衡学习
tags: 
- 不平衡学习与梯度下降
updated: 2018-04-17
---
 <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/title.png" width="600px" height="400px"/>

&nbsp;[原文链接](https://surface.syr.edu/cgi/viewcontent.cgi?article=1122&context=eecs_techreports)

# 前言

&nbsp;&nbsp;&nbsp;&nbsp;本文是最早较为深入研究不平衡学习问题的论文之一，发表于1991年。  
&nbsp;&nbsp;&nbsp;&nbsp;这篇文章给了我们一个deeper insight, 让我们知道了当训练神经网络时常用的**梯度下降法遇到不平衡二分类问题**时，其具体的参数更新过程是怎样的。
# 梯度下降法
## 不平衡数据中，应用梯度下降法的训练误差

 <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/a.png" width="600px" height="400px"/>
 
在不平衡二分类问题中，利用梯度下降法训练神经网络时，出现了如上图所示的现象：  
a.多数类的分类误差在前几次训练迭代中快速降到一个很小的值，之后其以很小的速率下降。  
b.少数类的分类误差在前几次训练迭代中不降反升，之后其以很小的速率下降。
 
## 实验现象解释
 对于图1的实验现象,作者基于下面的神经网络结构，给出了一定的解释。
 
  <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/b.png" width="600px" height="400px"/>
  
该网络的前向传播过程如下：

   <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/c.png" width="600px" height="400px"/>
      <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/d.png" width="600px" height="400px"/>

该网络的后向传播过程如下：  
对于隐藏层-输出层间的**某个权重**，对于输入的(j,k)样本，其相应的负梯度方向如下：
      <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/e.png" width="600px" height="400px"/>
      
对于输入层-隐藏层间的**某个权重**，对于输入的(j,k)样本，其相应的负梯度方向如下：
      <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/f.png" width="600px" height="400px"/>
   
故对于输入的(j,k)样本，其计算的**所有网络参数**的总的负梯度如下：
      <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/g.png" width="600px" height="400px"/>

故对于**某一类**的**所有**(j,k)样本,其相应的负梯度如下：  
<img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/h.png" width="400px" height="200px"/>

**原文作者提出的定理及其证明如下：**

 <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/i.png" width="600px" height="400px"/>
  <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/j.png" width="600px" height="400px"/> 
  
**(注意：本文这里设定输入数据X应被归一化到[0,1]范围内)** 

<img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/k.png" width="600px" height="400px"/>
  <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/l.png" width="600px" height="400px"/>  
  <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/m.png" width="600px" height="400px"/>

证明：作者对梯度长度的期望用了泰勒展开式，具体过程见原论文第10页以及附录部分。  
**注意，这里n1,n2应为对当前误差计算有贡献的多数类和少数类的相应样本数？**

**根据作者提出的三个定理，现在我们能够解释图1的实验现象了：**
  
  <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/n.png" width="600px" height="400px"/>

>1.如上图所示，多数类的负梯度为AC，少数类的负梯度为AB，它们合成的总更新梯度为AD。
可以看到，AD在AC上的分解(AF)与AC方向一致，但是AD在AB方向上的分解(AE)与AB相反。
故图1中，多数类的误差在**前几次迭代**中会快速下降，但少数类的误差不降反升。但在后面的迭代过程中，由于多数类的多数样本计算的误差接近于0,故总的负梯度方向在少数类负梯度方向上的分解方向，与少数类负梯度方向一致。故少数类的误差也会在后续迭代中下降。

>2.对于多数类来说，由于其后面预测的z与标签t已经很接近了，故其后面的迭代过程中，其误差已经变得很小。而且，由于z与t已经很接近了，故后向传播过程中的负梯度也很小，使得参数更新幅度不大，故多数类的误差降低的幅度也变得很小了。  
对于少数类来说，由于前面几次迭代中参数往其负梯度方向的反方向更新，故其分类误差很大，导致其对于权重更新的贡献也就很小，具体解释如下：

  <img src="{{ site.url }}/assets//blog_images//2018-04-17-不平衡二分类问题下的梯度下降(1991)/o.png" width="600px" height="400px"/>

