---
title: L-GEM分解成多个优化目标训练MLPNN(2016)
categories: 
- 论文笔记之L-GEM
tags: 
- L-GEM与神经网络
updated: 2018-05-05
---
 <img src="{{ site.url }}/assets//blog_images/L-GEM分解成多个优化目标训练MLPNN(2016)/title.png" width="600px" height="400px"/>

&nbsp;[原文链接](https://ieeexplore.ieee.org/document/7116585/)

# 1.简介
L-GEM能够较好地描述分类器的范化能力，分类器的在训练数据集上的训练误差以及ST-SM越小，其范化能力则越强。但是，以往的文章，是把训练误差和ST-SM作为单个优化目标，而实验中训练误差的值往往比ST-SM要大，这样就会造成优化过程中，训练误差占主导的情况。故本文把L-GEM分解成训练误差和ST-SM两个优化目标，采用基于帕累托[1]的多目标优化方法NSGA-II[2]来寻找最优的网络结构及其相应权重，同时采用iRprop+[3]算法加速优化的权重值的搜索过程。
# 2.算法

 <img src="{{ site.url }}/assets//blog_images//L-GEM分解成多个优化目标训练MLPNN(2016)/a.png" width="600px" height="400px"/>
 
 <img src="{{ site.url }}/assets//blog_images//L-GEM分解成多个优化目标训练MLPNN(2016)/b.png" width="600px" height="400px"/>
 
 <img src="{{ site.url }}/assets//blog_images//L-GEM分解成多个优化目标训练MLPNN(2016)/c.png" width="600px" height="400px"/>
 
 <img src="{{ site.url }}/assets//blog_images//L-GEM分解成多个优化目标训练MLPNN(2016)/d.png" width="600px" height="400px"/>


iRprop+的思想大体如下：

&nbsp;&nbsp;1.如果当前梯度方向与前一时刻梯度方向一致，则加大梯度更新步长;  
&nbsp;&nbsp;2.如果当前梯度方向与前一时刻梯度方向相反(越过了local minimal)，且相反的梯度方向使得E变大，那么就让下一时刻的权重重新回到前一时刻权重的位置(weight backtracking)，同时调小梯度更新步长;  
&nbsp;&nbsp;3.如果当前梯度方向与前一时刻梯度方向相反(越过了local minimal)，但E仍然是变小的，则不需要weight backtracking，只需调小梯度更新步长。

# 3.实验

 <img src="{{ site.url }}/assets//blog_images//L-GEM分解成多个优化目标训练MLPNN(2016)/e.png" width="600px" height="400px"/>

# 4.参考资料

[1][帕累托最优](https://zhuanlan.zhihu.com/p/26436967)

[2][NSGA-II多目标遗传算法](https://www.cnblogs.com/devilmaycry812839668/p/6268408.html)

[3][Empirical evaluation of the improved Rprop learning algorithms](https://ac.els-cdn.com/S0925231201007007/1-s2.0-S0925231201007007-main.pdf?_tid=73d8ff24-7136-40a7-9fbb-e4ef22f16894&acdnat=1526440207_593faa7587216113ef9dd309eb98ec64)

[4][Visualize Back Propagation: (6) RProp and iRProp+](https://www.youtube.com/watch?v=Cy2g9_hR-5Y)
 

